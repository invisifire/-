- Pearson’s Correlation,：皮尔逊相关系数，是用来度量 两个变量相互关系（线性相关）的，不过更多反应两个服从 正态分布的随机变量的相关性，取值范围在 [-1,+1] 之 间。
- Linear Discriminant Analysis(LDA，线性判别分析)：更 像一种特征抽取方式，基本思想是将高维的特征影到最佳鉴 别矢量空间，这样就可以抽取分类信息和达到压缩特征空 间维数的效果。投影后的样本在子空间有最大可分离性。
- Analysis of Variance：ANOVA,方差分析，通过分析研究不 同来源的变异对总变异的贡献大小，从而确定可控因素对研 究结果影响力的大小。
- Chi-Square：卡方检验，就是统计样本的实际观测值与理论 推断值之间的偏离程度，实际观测值与理论推断值之间的偏 离程 度就决定卡方值的大小，卡方值越大，越不符合；卡 方值越小，偏差越小，越趋于符合。


 

有不少特征筛选和特征过滤的共性，主要的特点就是通过不同的方法去计算不同特征对于模型的贡献。



方法：Lasso，Elastic Net，Ridge Regression，等。

 [机器学习之特征工程-特征选择-腾讯云开发者社区-腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1055767) 

**卡方检验**



只能用于二分类，经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量：

![img](https://ask.qcloudimg.com/http-save/yehe-1332428/4jtxddzrp4.png?imageView2/2/w/1200)

不难发现，这个统计量的含义简而言之就是自变量对因变量的相关性

**互信息法**

互信息系数能够很好地度量各种相关性，但是计算相对复杂一些，互信息计算公式如下：